{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0oyPnEQIxpq"
      },
      "source": [
        "# Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE0qteMlufaZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn.linear_model as lm\n",
        "from statsmodels.stats.weightstats import ttest_ind\n",
        "from scipy import stats\n",
        "from google.colab import drive\n",
        "import statsmodels.api as sm\n",
        "import statsmodels.formula.api as smf\n",
        "from datetime import datetime\n",
        "from sklearn.utils import resample\n",
        "from random import choices\n",
        "from statsmodels.stats.multitest import multipletests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwtx6H3aAHvv",
        "outputId": "c5a3a9d3-0b89-45b7-d633-eb5eaa25d7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stargazer\n",
            "  Downloading stargazer-0.0.7-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: stargazer\n",
            "Successfully installed stargazer-0.0.7\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  from stargazer.stargazer import Stargazer\n",
        "except ImportError:\n",
        "  %pip install stargazer\n",
        "  from stargazer.stargazer import Stargazer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgzJ03RhI4Go"
      },
      "source": [
        "# Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "iuRkzWxYXh3r",
        "outputId": "acde07d4-ad20-4a92-c495-da69a3b9f46d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/project_3_econ148/data/baseline.dta'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ccd02cce57e6>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# This is Nabeel, I renamed the file path in these cells so i could access them from my drive.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_stata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/project_3_econ148/data/baseline.dta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbok_inflation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_stata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/project_3_econ148/data/BOK_inflation.dta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcleanpricedata_y1y2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_stata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/project_3_econ148/data/cleanPriceData_Y1Y2.dta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mintensity_obs_short\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_stata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/project_3_econ148/data/intensity_obs_short.dta'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/stata.py\u001b[0m in \u001b[0;36mread_stata\u001b[0;34m(filepath_or_buffer, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, iterator, compression, storage_options)\u001b[0m\n\u001b[1;32m   2088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2089\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2090\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2091\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/stata.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals)\u001b[0m\n\u001b[1;32m   1700\u001b[0m         \u001b[0morder_categoricals\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1701\u001b[0m     ) -> DataFrame:\n\u001b[0;32m-> 1702\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1703\u001b[0m         \u001b[0;31m# Handle empty file or chunk.  If reading incrementally raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m         \u001b[0;31m# StopIteration.  If reading the whole thing return an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/stata.py\u001b[0m in \u001b[0;36m_ensure_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \"\"\"\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_path_or_buf\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/stata.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m             )\n\u001b[0;32m-> 1189\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m   1190\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_path_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/project_3_econ148/data/baseline.dta'"
          ]
        }
      ],
      "source": [
        "# This is Nabeel, I renamed the file path in these cells so i could access them from my drive.\n",
        "baseline = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/baseline.dta')\n",
        "bok_inflation = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/BOK_inflation.dta')\n",
        "cleanpricedata_y1y2 = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/cleanPriceData_Y1Y2.dta')\n",
        "intensity_obs_short = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/intensity_obs_short.dta')\n",
        "lrfu_select_dataset = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/LRFU_select_dataset.dta')\n",
        "ms1ms2_pooled = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/MS1MS2_pooled.dta')\n",
        "repayment_datay1 = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/repayment_dataY1.dta')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtyS2XfKKJKd",
        "outputId": "1b99299a-5083-41a6-a9d6-7a27caaa9f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive if not already mounted\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# data should be stored in an folder directly under the main drive such that the files are store under '/content/drive/MyDrive/project_3_econ148/data/' for it to import the data correctly\n",
        "baseline = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/baseline.dta')\n",
        "bok_inflation = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/BOK_inflation.dta')\n",
        "cleanpricedata_y1y2 = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/cleanPriceData_Y1Y2.dta')\n",
        "intensity_obs_short = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/intensity_obs_short.dta')\n",
        "lrfu_select_dataset = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/LRFU_select_dataset.dta')\n",
        "ms1ms2_pooled = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/MS1MS2_pooled.dta')\n",
        "repayment_datay1 = pd.read_stata('/content/drive/MyDrive/project_3_econ148/data/repayment_dataY1.dta')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ufa1dpjdPM4N"
      },
      "source": [
        "# Recreating the tables from the paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLR-EsZmPhnZ"
      },
      "source": [
        "## Table 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDod8jI8PmcV"
      },
      "source": [
        "We start by cleaning the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27C6MkYfPjWZ",
        "outputId": "2f98a106-deb1-4d0c-b081-330a8f6312ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1019\n"
          ]
        }
      ],
      "source": [
        "# clean ms1ms2_pooled (drop if MS !=2, keep columns oafid and treatMS1MS2, group by oafid and take mean and rename)\n",
        "ms1ms2_pooled_tab1 = ms1ms2_pooled[ms1ms2_pooled['MS']==2]\n",
        "ms1ms2_pooled_tab1 = ms1ms2_pooled_tab1[['oafid', 'treatMS1MS2']]\n",
        "ms1ms2_pooled_tab1 = ms1ms2_pooled_tab1.groupby('oafid', as_index=False).mean()\n",
        "ms1ms2_pooled_tab1.rename(columns={'treatMS1MS2': 'treat13'}, inplace=True)\n",
        "print(ms1ms2_pooled_tab1.shape[0]) # checking we have the right number of observations as described in the original article"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDAHBMLAPxQM"
      },
      "outputs": [],
      "source": [
        "# clean baseline data (the stata code indicates that the variables columns 'businessprofitmonth' and 'delta' should be kept, however they have already been renamed to 'businessprofitmonth_base' and 'delta_base')\n",
        "base_cols = ['oafid', 'logtotcons_base', 'male', 'num_adults', 'num_schoolchildren', 'finished_primary',\n",
        "                   'finished_secondary', 'cropland', 'num_rooms', 'schoolfees', 'totcons_base', 'logpercapcons_base',\n",
        "                   'total_cash_savings_base', 'total_cash_savings_trimmed', 'has_savings_acct', 'taken_bank_loan',\n",
        "                   'taken_informal_loan', 'liquidWealth', 'wagepay', 'businessprofitmonth_base', 'price_avg_diff_pct',\n",
        "                   'price_expect_diff_pct', 'harvest2011', 'netrevenue2011', 'netseller2011', 'autarkic2011',\n",
        "                   'maizelostpct2011', 'harvest2012', 'correct_interest', 'digit_recall', 'maizegiver', 'delta_base', 'treatment']\n",
        "baseline_clean = baseline[base_cols].copy()\n",
        "\n",
        "# rename columns\n",
        "baseline_clean.columns = [col + '_base' if not col.endswith('_base') and col != 'oafid' and col != 'treatment' else col for col in baseline_clean.columns]\n",
        "baseline_clean.rename(columns={'treatment': 'treatment2012'}, inplace=True)\n",
        "\n",
        "# generate treat12 as bool for treatment and control in 2012\n",
        "baseline_clean['treat12'] = baseline_clean['treatment2012'].apply(lambda x: x in ['T1', 'T2'])\n",
        "baseline_clean.loc[baseline_clean['treatment2012'] == '', 'treat12'] = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NLEtoCbkPzAN"
      },
      "outputs": [],
      "source": [
        "# merge baseline_clean and ms1ms2_pooled_clean on oafid\n",
        "base_ms1ms2_pool = pd.merge(baseline_clean, ms1ms2_pooled_tab1, on='oafid', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69_xFPsuQTmG",
        "outputId": "32d7d179-a9f6-469d-9a8a-a26fb3bdffc9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrrrrr}\n",
            "\\toprule\n",
            "Baseline characteristic & Treat & Control & Obs & Std diff & P-val \\\\\n",
            "\\midrule\n",
            "Male & 0.296 & 0.334 & 1589 & -0.083 & 0.109 \\\\\n",
            "Number of adults & 3.004 & 3.196 & 1510 & -0.099 & 0.067 \\\\\n",
            "Children in school & 2.998 & 3.072 & 1589 & -0.038 & 0.454 \\\\\n",
            "Finished primary school & 0.718 & 0.772 & 1490 & -0.122 & 0.019 \\\\\n",
            "Finished secondary school & 0.253 & 0.270 & 1490 & -0.039 & 0.460 \\\\\n",
            "Total cropland (acres) & 2.441 & 2.398 & 1512 & 0.014 & 0.796 \\\\\n",
            "Number of rooms in household & 3.073 & 3.252 & 1511 & -0.072 & 0.219 \\\\\n",
            "Total school fees & 27239.693 & 29813.631 & 1589 & -0.068 & 0.191 \\\\\n",
            "Average monthly consumption (Ksh) & 14970.862 & 15371.378 & 1437 & -0.032 & 0.550 \\\\\n",
            "Average monthly consumption/capita (log) & 7.975 & 7.963 & 1434 & 0.019 & 0.721 \\\\\n",
            "Total cash savings (Ksh) & 5157.396 & 8021.499 & 1572 & -0.128 & 0.028 \\\\\n",
            "Total cash savings (trim) & 4731.623 & 5389.836 & 1572 & -0.050 & 0.343 \\\\\n",
            "Has bank savings acct & 0.419 & 0.425 & 1589 & -0.012 & 0.815 \\\\\n",
            "Taken bank loan & 0.079 & 0.083 & 1589 & -0.018 & 0.730 \\\\\n",
            "Taken informal loan & 0.244 & 0.249 & 1589 & -0.011 & 0.836 \\\\\n",
            "Liquid wealth (Ksh) & 93878.938 & 97280.922 & 1491 & -0.032 & 0.547 \\\\\n",
            "Off-farm wages (Ksh) & 3916.817 & 3797.480 & 1589 & 0.010 & 0.854 \\\\\n",
            "Business profit (Ksh) & 2302.588 & 1801.685 & 1589 & 0.051 & 0.265 \\\\\n",
            "Avg $\\%\\Delta$ price Sep-Jun & 133.495 & 133.178 & 1504 & 0.004 & 0.939 \\\\\n",
            "Expect $\\%\\Delta$ price Sep12-Jun13 & 124.680 & 117.255 & 1510 & 0.075 & 0.103 \\\\\n",
            "2011 LR harvest (bags) & 9.364 & 9.025 & 1511 & 0.022 & 0.670 \\\\\n",
            "Net revenue 2011 (Ksh) & -3303.691 & -4088.622 & 1428 & 0.017 & 0.716 \\\\\n",
            "Net seller 2011 & 0.324 & 0.303 & 1428 & 0.046 & 0.393 \\\\\n",
            "Autarkic 2011 & 0.068 & 0.060 & 1589 & 0.034 & 0.506 \\\\\n",
            "\\% maize lost 2011 & 0.016 & 0.013 & 1428 & 0.030 & 0.563 \\\\\n",
            "2012 LR harvest (bags) & 11.181 & 11.030 & 1484 & 0.018 & 0.733 \\\\\n",
            "Calculated interest correctly & 0.715 & 0.730 & 1580 & -0.034 & 0.502 \\\\\n",
            "Digit span recall & 4.568 & 4.576 & 1504 & -0.007 & 0.890 \\\\\n",
            "Maize giver & 0.261 & 0.261 & 1589 & -0.001 & 0.985 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# create table 1\n",
        "# copy in case we need this later\n",
        "df_tab1 = base_ms1ms2_pool.copy()\n",
        "df_tab1['schoolfees_base'] = df_tab1['schoolfees_base']*1000\n",
        "\n",
        "# var list for table 1\n",
        "vars_list = [\n",
        "    \"male_base\", \"num_adults_base\", \"num_schoolchildren_base\", \"finished_primary_base\",\n",
        "    \"finished_secondary_base\", \"cropland_base\", \"num_rooms_base\", \"schoolfees_base\",\n",
        "    \"totcons_base\", \"logpercapcons_base\", \"total_cash_savings_base\",\n",
        "    \"total_cash_savings_trimmed_base\", \"has_savings_acct_base\", \"taken_bank_loan_base\",\n",
        "    \"taken_informal_loan_base\", \"liquidWealth_base\", \"wagepay_base\",\n",
        "    \"businessprofitmonth_base\", \"price_avg_diff_pct_base\",\n",
        "    \"price_expect_diff_pct_base\", \"harvest2011_base\", \"netrevenue2011_base\",\n",
        "    \"netseller2011_base\", \"autarkic2011_base\", \"maizelostpct2011_base\",\n",
        "    \"harvest2012_base\", \"correct_interest_base\", \"digit_recall_base\",\n",
        "    \"maizegiver_base\"\n",
        "]\n",
        "\n",
        "renaming = {\n",
        "    \"male_base\": \"Male\",\n",
        "    \"num_adults_base\": \"Number of adults\",\n",
        "    \"num_schoolchildren_base\": \"Children in school\",\n",
        "    \"finished_primary_base\": \"Finished primary school\",\n",
        "    \"finished_secondary_base\": \"Finished secondary school\",\n",
        "    \"cropland_base\": \"Total cropland (acres)\",\n",
        "    \"num_rooms_base\": \"Number of rooms in household\",\n",
        "    \"schoolfees_base\": \"Total school fees\",\n",
        "    \"totcons_base\": \"Average monthly consumption (Ksh)\",\n",
        "    \"logpercapcons_base\": \"Average monthly consumption/capita (log)\",\n",
        "    \"total_cash_savings_base\": \"Total cash savings (Ksh)\",\n",
        "    \"total_cash_savings_trimmed_base\": \"Total cash savings (trim)\",\n",
        "    \"has_savings_acct_base\": \"Has bank savings acct\",\n",
        "    \"taken_bank_loan_base\": \"Taken bank loan\",\n",
        "    \"taken_informal_loan_base\": \"Taken informal loan\",\n",
        "    \"liquidWealth_base\": \"Liquid wealth (Ksh)\",\n",
        "    \"wagepay_base\": \"Off-farm wages (Ksh)\",\n",
        "    \"businessprofitmonth_base\": \"Business profit (Ksh)\",\n",
        "    \"price_avg_diff_pct_base\": \"Avg $\\%\\Delta$ price Sep-Jun\",\n",
        "    \"price_expect_diff_pct_base\": \"Expect $\\%\\Delta$ price Sep12-Jun13\",\n",
        "    \"harvest2011_base\": \"2011 LR harvest (bags)\",\n",
        "    \"netrevenue2011_base\": \"Net revenue 2011 (Ksh)\",\n",
        "    \"netseller2011_base\": \"Net seller 2011\",\n",
        "    \"autarkic2011_base\": \"Autarkic 2011\",\n",
        "    \"maizelostpct2011_base\": \"\\% maize lost 2011\",\n",
        "    \"harvest2012_base\": \"2012 LR harvest (bags)\",\n",
        "    \"correct_interest_base\": \"Calculated interest correctly\",\n",
        "    \"digit_recall_base\": \"Digit span recall\",\n",
        "    \"maizegiver_base\": \"Maize giver\"\n",
        "}\n",
        "\n",
        "# function to perform t-tests\n",
        "def t_test_by_group(df, var, group_var='treat12'):\n",
        "    group1 = df[df[group_var] == 0][var].dropna()\n",
        "    group2 = df[df[group_var] == 1][var].dropna()\n",
        "    t_stat, p_val = stats.ttest_ind(group1, group2, equal_var=False)\n",
        "    return group1.mean(), group2.mean(), len(group1) + len(group2), t_stat, p_val\n",
        "\n",
        "# applying t-tests and collecting results\n",
        "results = []\n",
        "for var in vars_list:\n",
        "    control_mean, treat_mean, obs, t_stat, p_val = t_test_by_group(df_tab1, var)\n",
        "    std_diff = (treat_mean - control_mean) / np.sqrt(((len(df_tab1[df_tab1['treat12'] == 0][var]) - 1) * np.std(df_tab1[df_tab1['treat12'] == 0][var], ddof=1) ** 2 + (len(df_tab1[df_tab1['treat12'] == 1][var]) - 1) * np.std(df_tab1[df_tab1['treat12'] == 1][var], ddof=1) ** 2) / (len(df_tab1[df_tab1['treat12'] == 0][var]) + len(df_tab1[df_tab1['treat12'] == 1][var]) - 2))\n",
        "    results.append([var, treat_mean, control_mean, obs, std_diff, p_val])\n",
        "\n",
        "# convert results to a df to use pandas output to latex\n",
        "results_df = pd.DataFrame(results, columns=['Variable', 'Treat Mean', 'Control Mean', 'Observations', 'Std Diff', 'P-value'])\n",
        "results_df['Variable'] = results_df['Variable'].map(renaming)\n",
        "results_df = results_df.rename(columns={\n",
        "    'Variable':'Baseline characteristic',\n",
        "    'Treat Mean':'Treat',\n",
        "    'Control Mean':'Control',\n",
        "    'Observations':'Obs',\n",
        "    'Std Diff':'Std diff',\n",
        "    'P-value':'P-val'})\n",
        "\n",
        "latex_table1 = results_df.to_latex(index=False, float_format=\"%.3f\")\n",
        "print(latex_table1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2Kt7J_ve0Jr"
      },
      "source": [
        "## Table 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X1_xA0QR_xZ"
      },
      "outputs": [],
      "source": [
        "# year 1 overall\n",
        "ms1ms2_clean1 = ms1ms2_pooled.loc[:, ['treat12', 'interviewdate1', 'interviewdate', 'Y1round1', 'Y1round2', 'Y1round3', 'treatMS1MS2', 'inventory_trim', 'groupnum', 'strata_group', 'round']].dropna()\n",
        "ms1ms2_clean1['inter_R1'] = ms1ms2_clean1['Y1round1'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R2'] = ms1ms2_clean1['Y1round2'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R3'] = ms1ms2_clean1['Y1round3'] * ms1ms2_clean1['treat12']\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('inventory_trim ~ treat12 + interviewdate + C(strata_group)', data=ms1ms2_clean1)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results1 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params1 = results1.params\n",
        "model_pvalues1 = results1.pvalues\n",
        "mean_dv1 = ms1ms2_clean1['inventory_trim'].mean()\n",
        "sd_dv1 = ms1ms2_clean1['inventory_trim'].std()\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues1 = {key: multipletests(results1.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues1.items() if key in ['treat12', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrx7OVblFQI5"
      },
      "outputs": [],
      "source": [
        "# year 1 by round\n",
        "model_interactions = smf.ols('inventory_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(strata_group)', data=ms1ms2_clean1)\n",
        "results2 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params2 = results2.params\n",
        "model_pvalues2 = results2.pvalues\n",
        "mean_dv2 = ms1ms2_clean1['inventory_trim'].mean()\n",
        "sd_dv2 = ms1ms2_clean1['inventory_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params2) + ('inter_R2' in model_params2) + ('inter_R3' in model_params2)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues2 = {key: multipletests(results2.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues2.items() if key in ['treat12', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9om7TsFHfcnZ"
      },
      "outputs": [],
      "source": [
        "# year 2 overall\n",
        "ms1ms2_clean2 = ms1ms2_pooled.loc[:, ['treat13', 'Y2round1', 'Y2round2', 'Y2round3', 'treatMS1MS2', 'inventory_trim', 'interviewdate', 'date', 'strata_group', 'groupnum']].dropna()\n",
        "ms1ms2_clean2['inter_R1'] = ms1ms2_clean2['Y2round1'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R2'] = ms1ms2_clean2['Y2round2'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R3'] = ms1ms2_clean2['Y2round3'] * ms1ms2_clean2['treat13']\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('inventory_trim ~ treat13 + interviewdate + C(strata_group)', data=ms1ms2_clean2)\n",
        "\n",
        "# Fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results3 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params3 = results3.params\n",
        "model_pvalues3 = results3.pvalues\n",
        "mean_dv3 = ms1ms2_clean2['inventory_trim'].mean()\n",
        "sd_dv3 = ms1ms2_clean2['inventory_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params3) + ('inter_R2' in model_params3) + ('inter_R3' in model_params3)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues3 = {key: multipletests(results3.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues3.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6B_uPtaZstiu"
      },
      "outputs": [],
      "source": [
        "# year 2 by round\n",
        "model_interactions = smf.ols('inventory_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean2)\n",
        "results4 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params4 = results4.params\n",
        "model_pvalues4 = results4.pvalues\n",
        "mean_dv4 = ms1ms2_clean2['inventory_trim'].mean()\n",
        "sd_dv4 = ms1ms2_clean2['inventory_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params4) + ('inter_R2' in model_params4) + ('inter_R3' in model_params4)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues4 = {key: multipletests(results4.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues4.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q01gay6Tm4ts"
      },
      "outputs": [],
      "source": [
        "# pooled overall\n",
        "ms1ms2_clean3 = pd.concat([ms1ms2_clean1, ms1ms2_clean2], ignore_index=True).fillna(0)\n",
        "ms1ms2_clean3.sort_values(by='interviewdate')\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('inventory_trim ~ treatMS1MS2 + interviewdate + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results5 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params5 = results5.params\n",
        "model_pvalues5 = results5.pvalues\n",
        "mean_dv5 = ms1ms2_clean3['inventory_trim'].mean()\n",
        "sd_dv5 = ms1ms2_clean3['inventory_trim'].std()\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params5) + ('inter_R2' in model_params5) + ('inter_R3' in model_params5)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues5 = {key: multipletests(results5.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues5.items() if key in ['treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mw77nJqfxRK4"
      },
      "outputs": [],
      "source": [
        "# pooled by round\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('inventory_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results6 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "# Extract necessary statistics\n",
        "model_params6 = results6.params\n",
        "model_pvalues6 = results6.pvalues\n",
        "mean_dv6 = ms1ms2_clean3['inventory_trim'].mean()\n",
        "sd_dv6 = ms1ms2_clean3['inventory_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params6) + ('inter_R2' in model_params6) + ('inter_R3' in model_params6)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues6 = {key: multipletests(results6.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues6.items() if key in ['treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4XchsWdCzhq"
      },
      "outputs": [],
      "source": [
        "# collecting p-values in tables to easily add them to the latex table\n",
        "results_tab2 = [results1,results2,results3,results4,results5,results6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values = {}\n",
        "for result in results_tab2:\n",
        "    p_values[result] = np.round(result.pvalues,3)\n",
        "    p_values_df= pd.DataFrame(p_values)\n",
        "\n",
        "p_values_df = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']]\n",
        "p_values_df.columns = ['(1)', '(2)', '(3)', '(4)', '(5)', '(6)']\n",
        "p_values_df.loc['Treat'] = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "# fwer correction\n",
        "fwer_p_values = [bonferroni_pvalues1, bonferroni_pvalues2, bonferroni_pvalues3, bonferroni_pvalues4, bonferroni_pvalues5, bonferroni_pvalues6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values_fwer = {}\n",
        "for i, p_values in enumerate(fwer_p_values):\n",
        "    p_values_fwer[i] = p_values\n",
        "    p_values_df_fwer = pd.DataFrame(p_values_fwer)\n",
        "\n",
        "p_values_df_fwer.loc['Treat'] = p_values_df_fwer.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df_fwer.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "# changeing everything to numeric and rounding\n",
        "def to_numeric(x):\n",
        "    if isinstance(x, np.ndarray) and x.size == 1:\n",
        "        return x.item()  # Converts a one-element array to a scalar\n",
        "    return x\n",
        "\n",
        "p_values_df_fwer = p_values_df_fwer.applymap(to_numeric).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6La6m6trvPF",
        "outputId": "f3ab25ca-5be3-4fda-b931-4c0d67d1228c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lcccccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "& \\multicolumn{6}{c}{\\textit{Dependent variable: Inventory Trim}} \\\n",
            "\\cr \\cline{2-7}\n",
            "\\\\[-1.8ex] & \\multicolumn{2}{c}{Y1} & \\multicolumn{2}{c}{Y2} & \\multicolumn{2}{c}{Pooled}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \n",
            " \\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Treat & 0.574$^{***}$ & & & & & \\\\\n",
            "& (0.140) & & & & & \\\\\n",
            " Treat & & & 0.546$^{***}$ & & & \\\\\n",
            "& & & (0.129) & & & \\\\\n",
            " Treat & & & & & 0.565$^{***}$ & \\\\\n",
            "& & & & & (0.097) & \\\\\n",
            " Treat - R1 & & 0.872$^{***}$ & & 1.242$^{***}$ & & 1.050$^{***}$ \\\\\n",
            "& & (0.276) & & (0.235) & & (0.184) \\\\\n",
            " Treat - R2 & & 0.753$^{***}$ & & 0.304$^{*}$ & & 0.546$^{***}$ \\\\\n",
            "& & (0.171) & & (0.166) & & (0.120) \\\\\n",
            " Treat - R3 & & 0.111$^{}$ & & 0.082$^{}$ & & 0.094$^{}$ \\\\\n",
            "& & (0.083) & & (0.344) & & (0.162) \\\\\n",
            " P-Val Treat & 0.0 &  & 0.0 &  & 0.0 &  \\\\\n",
            " P-Val Treat FWER & 0.0 &  & 0.0 &  & 0.0 &  \\\\\n",
            " P-Val Treat - R1 &  & 0.002 &  & 0.0 &  & 0.0 \\\\\n",
            " P-Val Treat - R1 FWER &  & 0.002 &  & 0.0 &  & 0.0 \\\\\n",
            " P-Val Treat - R2 &  & 0.0 &  & 0.066 &  & 0.0 \\\\\n",
            " P-Val Treat - R2 FWER &  & 0.0 &  & 0.066 &  & 0.0 \\\\\n",
            " P-Val Treat - R3 &  & 0.182 &  & 0.812 &  & 0.561 \\\\\n",
            " P-Val Treat - R3 FWER &  & 0.182 &  & 0.812 &  & 0.561 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 3836 & 3836 & 2944 & 2944 & 6780 & 6780 \\\\\n",
            " $R^2$ & 0.365 & 0.368 & 0.098 & 0.215 & 0.144 & 0.329 \\\\\n",
            " % Adjusted $R^2$ & 0.360 & 0.362 & 0.088 & 0.205 & 0.136 & 0.322 \\\\\n",
            " % Residual Std. Error & 2.982 (df=3803) & 2.975 (df=3799) & 2.950 (df=2911) & 2.754 (df=2907) & 3.256 (df=6716) & 2.884 (df=6710) \\\\\n",
            " % F Statistic & 69.431$^{***}$ (df=32; 3803) & 84.886$^{***}$ (df=36; 3799) & 64.996$^{***}$ (df=32; 2911) & 57.574$^{***}$ (df=36; 2907) & 52.508$^{***}$ (df=63; 6716) & 56.147$^{***}$ (df=69; 6710) \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{6}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 63, but rank is 62\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 71, but rank is 68\n",
            "  warnings.warn('covariance of constraints does not have full '\n"
          ]
        }
      ],
      "source": [
        "# creating latex table\n",
        "stargazer_tab2 = Stargazer(results_tab2)\n",
        "\n",
        "stargazer_tab2.custom_columns(['Y1', 'Y2','Pooled'], [2,2,2])\n",
        "stargazer_tab2.significant_digits(3)\n",
        "stargazer_tab2.rename_covariates({'treat12': 'Treat','treat13': 'Treat', 'treatMS1MS2': 'Treat', 'inter_R1': 'Treat - R1', 'inter_R2': 'Treat - R2', 'inter_R3': 'Treat - R3'})\n",
        "stargazer_tab2.covariate_order(['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3'])\n",
        "# adding p-values\n",
        "stargazer_tab2.add_line('P-Val Treat',p_values_df.loc['Treat'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat FWER',p_values_df_fwer.loc['Treat'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R1',p_values_df.loc['inter_R1'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R1 FWER',p_values_df_fwer.loc['inter_R1'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R2',p_values_df.loc['inter_R2'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R2 FWER',p_values_df_fwer.loc['inter_R2'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R3',p_values_df.loc['inter_R3'].tolist())\n",
        "stargazer_tab2.add_line('P-Val Treat - R3 FWER',p_values_df_fwer.loc['inter_R3'].tolist())\n",
        "\n",
        "\n",
        "latex_table2 = stargazer_tab2.render_latex()\n",
        "\n",
        "latex_table2 = latex_table2.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\n \\\\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\")\n",
        "latex_table2 = latex_table2.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table2 = latex_table2.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table2 = latex_table2.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table2 = latex_table2.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
        "latex_table2 = latex_table2.replace(\"nan\",\"\")\n",
        "latex_table2 = latex_table2.replace(\"inventory_trim\",\"Inventory Trim\")\n",
        "latex_table2 = latex_table2.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
        "latex_table2 = latex_table2.replace(\"\\\\end{table}\", \"\")\n",
        "\n",
        "print(latex_table2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MapKqTx56PLs"
      },
      "source": [
        "## Table 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUZo0TFS6V_S"
      },
      "outputs": [],
      "source": [
        "# year 1 overall\n",
        "ms1ms2_clean1 = ms1ms2_pooled.loc[:, ['treat12', 'interviewdate1', 'interviewdate', 'Y1round1', 'Y1round2', 'Y1round3', 'treatMS1MS2', 'netrevenue_trim', 'groupnum', 'strata_group', 'round']].dropna()\n",
        "ms1ms2_clean1['inter_R1'] = ms1ms2_clean1['Y1round1'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R2'] = ms1ms2_clean1['Y1round2'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R3'] = ms1ms2_clean1['Y1round3'] * ms1ms2_clean1['treat12']\n",
        "\n",
        "# Example of running a regression for 'inventory_trim' against 'treat12' with control\n",
        "# variables and fixed effects for strata_group and clustering by groupnum\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('netrevenue_trim ~ treat12 + interviewdate + C(strata_group)', data=ms1ms2_clean1)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results1 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params1 = results1.params\n",
        "model_pvalues1 = results1.pvalues\n",
        "mean_dv1 = ms1ms2_clean1['netrevenue_trim'].mean()\n",
        "sd_dv1 = ms1ms2_clean1['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params1) + ('inter_R2' in model_params1) + ('inter_R3' in model_params1)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues1 = {key: multipletests(results1.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues1.items() if key in ['treat12', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQhGrlEf66e1"
      },
      "outputs": [],
      "source": [
        "# year 1 by rounds\n",
        "# Similarly, for the models with the interactions (inter_Y1R1, inter_Y1R2, inter_Y1R3):\n",
        "model_interactions = smf.ols('netrevenue_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(strata_group)', data=ms1ms2_clean1)\n",
        "results2 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params2 = results2.params\n",
        "model_pvalues2 = results2.pvalues\n",
        "mean_dv2 = ms1ms2_clean1['netrevenue_trim'].mean()\n",
        "sd_dv2 = ms1ms2_clean1['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params2) + ('inter_R2' in model_params2) + ('inter_R3' in model_params2)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues2 = {key: multipletests(results2.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues2.items() if key in ['inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2S1aDTrk7kB3"
      },
      "outputs": [],
      "source": [
        "# year 2 overall\n",
        "ms1ms2_clean2 = ms1ms2_pooled.loc[:, ['treat13', 'Y2round1', 'Y2round2', 'Y2round3', 'treatMS1MS2', 'netrevenue_trim', 'interviewdate', 'date', 'strata_group', 'groupnum']].dropna()\n",
        "ms1ms2_clean2['inter_R1'] = ms1ms2_clean2['Y2round1'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R2'] = ms1ms2_clean2['Y2round2'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R3'] = ms1ms2_clean2['Y2round3'] * ms1ms2_clean2['treat13']\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('netrevenue_trim ~ treat13 + interviewdate + C(strata_group)', data=ms1ms2_clean2)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results3 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params3 = results3.params\n",
        "model_pvalues3 = results3.pvalues\n",
        "mean_dv3 = ms1ms2_clean2['netrevenue_trim'].mean()\n",
        "sd_dv3 = ms1ms2_clean2['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params3) + ('inter_R2' in model_params3) + ('inter_R3' in model_params3)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues3 = {key: multipletests(results3.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues3.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeiW7KK18ZVa"
      },
      "outputs": [],
      "source": [
        "# year 2 by round\n",
        "# Similarly, for the models with the interactions (inter_Y1R1, inter_Y1R2, inter_Y1R3):\n",
        "model_interactions = smf.ols('netrevenue_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean2)\n",
        "results4 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "# results4.summary()\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params4 = results4.params\n",
        "model_pvalues4 = results4.pvalues\n",
        "mean_dv4 = ms1ms2_clean2['netrevenue_trim'].mean()\n",
        "sd_dv4 = ms1ms2_clean2['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params4) + ('inter_R2' in model_params4) + ('inter_R3' in model_params4)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues4 = {key: multipletests(results4.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues4.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84MnerBV_AFO"
      },
      "outputs": [],
      "source": [
        "# pooled overall\n",
        "ms1ms2_clean3 = pd.concat([ms1ms2_clean1, ms1ms2_clean2], ignore_index=True).fillna(0)\n",
        "ms1ms2_clean3.sort_values(by='interviewdate')\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('netrevenue_trim ~ treatMS1MS2 + interviewdate + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results5 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params5 = results5.params\n",
        "model_pvalues5 = results5.pvalues\n",
        "mean_dv5 = ms1ms2_clean3['netrevenue_trim'].mean()\n",
        "sd_dv5 = ms1ms2_clean3['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params5) + ('inter_R2' in model_params5) + ('inter_R3' in model_params5)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues5 = {key: multipletests(results5.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues5.items() if key in ['treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P729F96J_dkm"
      },
      "outputs": [],
      "source": [
        "# pooled by round\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('netrevenue_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results6 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params6 = results6.params\n",
        "model_pvalues6 = results6.pvalues\n",
        "mean_dv6 = ms1ms2_clean3['netrevenue_trim'].mean()\n",
        "sd_dv6 = ms1ms2_clean3['netrevenue_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params6) + ('inter_R2' in model_params6) + ('inter_R3' in model_params6)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues6 = {key: multipletests(results6.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues6.items() if key in ['inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9K-vg4NuEB3F"
      },
      "outputs": [],
      "source": [
        "# collecting p-values in tables to easily add them to the latex table\n",
        "results_tab3 = [results1,results2,results3,results4,results5,results6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values = {}\n",
        "for result in results_tab3:\n",
        "    p_values[result] = np.round(result.pvalues,3)\n",
        "    p_values_df= pd.DataFrame(p_values)\n",
        "\n",
        "p_values_df = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']]\n",
        "p_values_df.columns = ['(1)', '(2)', '(3)', '(4)', '(5)', '(6)']\n",
        "p_values_df.loc['Treat'] = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "# fwer correction\n",
        "fwer_p_values = [bonferroni_pvalues1, bonferroni_pvalues2, bonferroni_pvalues3, bonferroni_pvalues4, bonferroni_pvalues5, bonferroni_pvalues6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values_fwer = {}\n",
        "for i, p_values in enumerate(fwer_p_values):\n",
        "    p_values_fwer[i] = p_values\n",
        "    p_values_df_fwer = pd.DataFrame(p_values_fwer)\n",
        "\n",
        "p_values_df_fwer.loc['Treat'] = p_values_df_fwer.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df_fwer.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "\n",
        "p_values_df_fwer = p_values_df_fwer.applymap(to_numeric).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nW2kizOwEFRG",
        "outputId": "2c40e690-688a-4475-94f5-1372b3b1497f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lcccccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "& \\multicolumn{6}{c}{\\textit{Dependent variable: Netrevenue Trim}} \\\n",
            "\\cr \\cline{2-7}\n",
            "\\\\[-1.8ex] & \\multicolumn{2}{c}{Y1} & \\multicolumn{2}{c}{Y2} & \\multicolumn{2}{c}{Pooled}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \n",
            " \\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Treat & 263.790$^{}$ & & & & & \\\\\n",
            "& (255.661) & & & & & \\\\\n",
            " Treat & & & 854.114$^{***}$ & & & \\\\\n",
            "& & & (303.802) & & & \\\\\n",
            " Treat & & & & & 531.358$^{***}$ & \\\\\n",
            "& & & & & (196.315) & \\\\\n",
            " Treat - R1 & & -1164.574$^{***}$ & & 16.478$^{}$ & & -613.581$^{**}$ \\\\\n",
            "& & (322.956) & & (444.957) & & (271.653) \\\\\n",
            " Treat - R2 & & 509.851$^{}$ & & 1994.923$^{***}$ & & 1187.967$^{***}$ \\\\\n",
            "& & (446.928) & & (503.696) & & (337.460) \\\\\n",
            " Treat - R3 & & 1370.344$^{***}$ & & 565.438$^{}$ & & 998.665$^{***}$ \\\\\n",
            "& & (412.602) & & (403.307) & & (291.103) \\\\\n",
            " P-Val Treat & 0.302 &  & 0.005 &  & 0.007 &  \\\\\n",
            " P-Val Treat FWER & 0.302 &  & 0.005 &  & 0.007 &  \\\\\n",
            " P-Val Treat - R1 &  & 0.0 &  & 0.97 &  & 0.024 \\\\\n",
            " P-Val Treat - R1 FWER &  & 0.0 &  & 0.97 &  & 0.024 \\\\\n",
            " P-Val Treat - R2 &  & 0.254 &  & 0.0 &  & 0.0 \\\\\n",
            " P-Val Treat - R2 FWER &  & 0.254 &  & 0.0 &  & 0.0 \\\\\n",
            " P-Val Treat - R3 &  & 0.001 &  & 0.161 &  & 0.001 \\\\\n",
            " P-Val Treat - R3 FWER &  & 0.001 &  & 0.161 &  & 0.001 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 3795 & 3795 & 2935 & 2935 & 6730 & 6730 \\\\\n",
            " $R^2$ & 0.025 & 0.038 & 0.074 & 0.079 & 0.107 & 0.119 \\\\\n",
            " % Adjusted $R^2$ & 0.017 & 0.029 & 0.064 & 0.067 & 0.099 & 0.110 \\\\\n",
            " % Residual Std. Error & 6160.285 (df=3762) & 6123.381 (df=3758) & 6332.926 (df=2902) & 6321.436 (df=2898) & 6257.097 (df=6666) & 6218.002 (df=6660) \\\\\n",
            " % F Statistic & 4.652$^{***}$ (df=32; 3762) & 5.786$^{***}$ (df=36; 3758) & 30.217$^{***}$ (df=32; 2902) & 29.094$^{***}$ (df=36; 2898) & 27.156$^{***}$ (df=63; 6666) & 3807742491.274$^{***}$ (df=69; 6660) \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{6}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 63, but rank is 62\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 71, but rank is 68\n",
            "  warnings.warn('covariance of constraints does not have full '\n"
          ]
        }
      ],
      "source": [
        "# creating latex table\n",
        "stargazer_tab3 = Stargazer(results_tab3)\n",
        "\n",
        "stargazer_tab3.custom_columns(['Y1', 'Y2','Pooled'], [2,2,2])\n",
        "stargazer_tab3.significant_digits(3)\n",
        "stargazer_tab3.rename_covariates({'treat12': 'Treat','treat13': 'Treat', 'treatMS1MS2': 'Treat', 'inter_R1': 'Treat - R1', 'inter_R2': 'Treat - R2', 'inter_R3': 'Treat - R3'})\n",
        "stargazer_tab3.covariate_order(['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3'])\n",
        "# adding p-values\n",
        "stargazer_tab3.add_line('P-Val Treat',p_values_df.loc['Treat'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat FWER',p_values_df_fwer.loc['Treat'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R1',p_values_df.loc['inter_R1'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R1 FWER',p_values_df_fwer.loc['inter_R1'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R2',p_values_df.loc['inter_R2'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R2 FWER',p_values_df_fwer.loc['inter_R2'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R3',p_values_df.loc['inter_R3'].tolist())\n",
        "stargazer_tab3.add_line('P-Val Treat - R3 FWER',p_values_df_fwer.loc['inter_R3'].tolist())\n",
        "\n",
        "\n",
        "latex_table3 = stargazer_tab3.render_latex()\n",
        "\n",
        "latex_table3 = latex_table3.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\n \\\\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\")\n",
        "latex_table3 = latex_table3.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table3 = latex_table3.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table3 = latex_table3.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table3 = latex_table3.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
        "latex_table3 = latex_table3.replace(\"nan\",\"\")\n",
        "latex_table3 = latex_table3.replace(\"netrevenue_trim\",\"Netrevenue Trim\")\n",
        "latex_table3 = latex_table3.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
        "latex_table3 = latex_table3.replace(\"\\\\end{table}\", \"\")\n",
        "\n",
        "print(latex_table3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFcatyv4Dk08"
      },
      "source": [
        "## Table 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm-n7nFMDiSQ"
      },
      "outputs": [],
      "source": [
        "# year 1 overall\n",
        "ms1ms2_clean1 = ms1ms2_pooled.loc[:, ['treat12', 'interviewdate1', 'interviewdate', 'Y1round1', 'Y1round2', 'Y1round3', 'treatMS1MS2', 'logtotcons_trim', 'groupnum', 'strata_group', 'round']].dropna()\n",
        "ms1ms2_clean1['inter_R1'] = ms1ms2_clean1['Y1round1'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R2'] = ms1ms2_clean1['Y1round2'] * ms1ms2_clean1['treat12']\n",
        "ms1ms2_clean1['inter_R3'] = ms1ms2_clean1['Y1round3'] * ms1ms2_clean1['treat12']\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('logtotcons_trim ~ treat12 + interviewdate + C(strata_group)', data=ms1ms2_clean1)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results1 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# Extract necessary statistics\n",
        "model_params1 = results1.params\n",
        "model_pvalues1 = results1.pvalues\n",
        "mean_dv1 = ms1ms2_clean1['logtotcons_trim'].mean()\n",
        "sd_dv1 = ms1ms2_clean1['logtotcons_trim'].std()\n",
        "\n",
        "# Number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params1) + ('inter_R2' in model_params1) + ('inter_R3' in model_params1)\n",
        "\n",
        "# Calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues1 = {key: multipletests(results1.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues1.items() if key in ['treat12', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emC8iS0mNYWb"
      },
      "outputs": [],
      "source": [
        "# year 1 by round\n",
        "model_interactions = smf.ols('logtotcons_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(strata_group)', data=ms1ms2_clean1)\n",
        "results2 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean1['groupnum']})\n",
        "\n",
        "# Extract necessary statistics\n",
        "model_params2 = results2.params\n",
        "model_pvalues2 = results2.pvalues\n",
        "mean_dv2 = ms1ms2_clean1['logtotcons_trim'].mean()\n",
        "sd_dv2 = ms1ms2_clean1['logtotcons_trim'].std()\n",
        "\n",
        "# Number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params2) + ('inter_R2' in model_params2) + ('inter_R3' in model_params2)\n",
        "\n",
        "# Calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues2 = {key: multipletests(results2.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues2.items() if key in ['inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Jy74NvxNqmO"
      },
      "outputs": [],
      "source": [
        "# year 2 overall\n",
        "ms1ms2_clean2 = ms1ms2_pooled.loc[:, ['treat13', 'Y2round1', 'Y2round2', 'Y2round3', 'treatMS1MS2', 'logtotcons_trim', 'interviewdate', 'date', 'strata_group', 'groupnum']].dropna()\n",
        "ms1ms2_clean2['inter_R1'] = ms1ms2_clean2['Y2round1'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R2'] = ms1ms2_clean2['Y2round2'] * ms1ms2_clean2['treat13']\n",
        "ms1ms2_clean2['inter_R3'] = ms1ms2_clean2['Y2round3'] * ms1ms2_clean2['treat13']\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('logtotcons_trim ~ treat13 + interviewdate + C(strata_group)', data=ms1ms2_clean2)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results3 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "\n",
        "# Extract necessary statistics\n",
        "model_params3 = results3.params\n",
        "model_pvalues3 = results3.pvalues\n",
        "mean_dv3 = ms1ms2_clean2['logtotcons_trim'].mean()\n",
        "sd_dv3 = ms1ms2_clean2['logtotcons_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params3) + ('inter_R2' in model_params3) + ('inter_R3' in model_params3)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues3 = {key: multipletests(results3.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues3.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjdhLgKJNx38"
      },
      "outputs": [],
      "source": [
        "# year 2 by round\n",
        "model_interactions = smf.ols('logtotcons_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean2)\n",
        "results4 = model_interactions.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean2['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params4 = results4.params\n",
        "model_pvalues4 = results4.pvalues\n",
        "mean_dv4 = ms1ms2_clean2['logtotcons_trim'].mean()\n",
        "sd_dv4 = ms1ms2_clean2['logtotcons_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params4) + ('inter_R2' in model_params4) + ('inter_R3' in model_params4)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues4 = {key: multipletests(results4.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues4.items() if key in ['treat13', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhjKAz2sOUlj"
      },
      "outputs": [],
      "source": [
        "# pooled overall\n",
        "ms1ms2_clean3 = pd.concat([ms1ms2_clean1, ms1ms2_clean2], ignore_index=True).fillna(0)\n",
        "ms1ms2_clean3.sort_values(by='interviewdate')\n",
        "\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('logtotcons_trim ~ treatMS1MS2 + interviewdate + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results5 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params5 = results5.params\n",
        "observations5 = len(ms1ms2_clean3)\n",
        "mean_dv5 = ms1ms2_clean3['logtotcons_trim'].mean()\n",
        "sd_dv5 = ms1ms2_clean3['logtotcons_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params5) + ('inter_R2' in model_params5) + ('inter_R3' in model_params5)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues5 = {key: multipletests(results5.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues5.items() if key in ['treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8SlQkenSOeUc"
      },
      "outputs": [],
      "source": [
        "# pooled by round\n",
        "# model specification with fixed effects for strata_group\n",
        "model = smf.ols('logtotcons_trim ~ inter_R1 + inter_R2 + inter_R3 + interviewdate + C(Y1round1) + C(Y1round2) + C(Y1round3) + C(Y2round1) + C(Y2round2) + C(Y2round3) + C(strata_group)', data=ms1ms2_clean3)\n",
        "\n",
        "# fitting the model with robust standard errors clustered by 'groupnum'\n",
        "results6 = model.fit(cov_type='cluster', cov_kwds={'groups': ms1ms2_clean3['groupnum']})\n",
        "\n",
        "# extract necessary statistics\n",
        "model_params6 = results6.params\n",
        "model_pvalues6 = results6.pvalues\n",
        "mean_dv6 = ms1ms2_clean3['logtotcons_trim'].mean()\n",
        "sd_dv6 = ms1ms2_clean3['logtotcons_trim'].std()\n",
        "\n",
        "# number of tests, include only treat12 and its interaction terms if they exist\n",
        "num_tests = 1 + ('inter_R1' in model_params6) + ('inter_R2' in model_params6) + ('inter_R3' in model_params6)\n",
        "\n",
        "# calculating the Bonferroni corrections for the p-values\n",
        "bonferroni_pvalues6 = {key: multipletests(results6.pvalues[key], method='fdr_bh')[1] for key, val in model_pvalues6.items() if key in ['inter_R1', 'inter_R2', 'inter_R3']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wot_lavVE979"
      },
      "outputs": [],
      "source": [
        "# collecting p-values in tables to easily add them to the latex table\n",
        "results_tab4 = [results1,results2,results3,results4,results5,results6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values = {}\n",
        "for result in results_tab4:\n",
        "    p_values[result] = np.round(result.pvalues,3)\n",
        "    p_values_df= pd.DataFrame(p_values)\n",
        "\n",
        "p_values_df = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3']]\n",
        "p_values_df.columns = ['(1)', '(2)', '(3)', '(4)', '(5)', '(6)']\n",
        "p_values_df.loc['Treat'] = p_values_df.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "# fwer correction\n",
        "fwer_p_values = [bonferroni_pvalues1, bonferroni_pvalues2, bonferroni_pvalues3, bonferroni_pvalues4, bonferroni_pvalues5, bonferroni_pvalues6]\n",
        "\n",
        "# get p-values for treat12, treat13, treatMS1MS2, inter_R1, inter_R2, inter_R3\n",
        "p_values_fwer = {}\n",
        "for i, p_values in enumerate(fwer_p_values):\n",
        "    p_values_fwer[i] = p_values\n",
        "    p_values_df_fwer = pd.DataFrame(p_values_fwer)\n",
        "\n",
        "p_values_df_fwer.loc['Treat'] = p_values_df_fwer.loc[['treat12', 'treat13', 'treatMS1MS2']].mean()\n",
        "p_values_df_fwer.drop(['treat12', 'treat13', 'treatMS1MS2'], inplace=True)\n",
        "\n",
        "# changeing everything to numeric and rounding\n",
        "p_values_df_fwer = p_values_df_fwer.applymap(to_numeric).round(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQIGcHsYE_qW",
        "outputId": "fdbc6468-fc8c-4044-bd9e-80dc2bcb0a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lcccccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "& \\multicolumn{6}{c}{\\textit{Dependent variable: Log Total Consumption}} \\\n",
            "\\cr \\cline{2-7}\n",
            "\\\\[-1.8ex] & \\multicolumn{2}{c}{Y1} & \\multicolumn{2}{c}{Y2} & \\multicolumn{2}{c}{Pooled}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \n",
            " \\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Treat & 0.012$^{}$ & & & & & \\\\\n",
            "& (0.030) & & & & & \\\\\n",
            " Treat & & & 0.064$^{*}$ & & & \\\\\n",
            "& & & (0.036) & & & \\\\\n",
            " Treat & & & & & 0.036$^{}$ & \\\\\n",
            "& & & & & (0.023) & \\\\\n",
            " Treat - R1 & & -0.033$^{}$ & & 0.064$^{}$ & & 0.013$^{}$ \\\\\n",
            "& & (0.047) & & (0.047) & & (0.033) \\\\\n",
            " Treat - R2 & & 0.028$^{}$ & & 0.076$^{*}$ & & 0.049$^{*}$ \\\\\n",
            "& & (0.039) & & (0.043) & & (0.029) \\\\\n",
            " Treat - R3 & & 0.038$^{}$ & & 0.052$^{}$ & & 0.044$^{}$ \\\\\n",
            "& & (0.042) & & (0.047) & & (0.031) \\\\\n",
            " P-Val Treat & 0.683 &  & 0.08 &  & 0.126 &  \\\\\n",
            " P-Val Treat FWER & 0.683 &  & 0.08 &  & 0.126 &  \\\\\n",
            " P-Val Treat - R1 &  & 0.486 &  & 0.171 &  & 0.687 \\\\\n",
            " P-Val Treat - R1 FWER &  & 0.486 &  & 0.171 &  & 0.687 \\\\\n",
            " P-Val Treat - R2 &  & 0.481 &  & 0.075 &  & 0.088 \\\\\n",
            " P-Val Treat - R2 FWER &  & 0.481 &  & 0.075 &  & 0.088 \\\\\n",
            " P-Val Treat - R3 &  & 0.364 &  & 0.27 &  & 0.164 \\\\\n",
            " P-Val Treat - R3 FWER &  & 0.364 &  & 0.27 &  & 0.164 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 3792 & 3792 & 2944 & 2944 & 6736 & 6736 \\\\\n",
            " $R^2$ & 0.026 & 0.027 & 0.051 & 0.053 & 0.055 & 0.056 \\\\\n",
            " % Adjusted $R^2$ & 0.018 & 0.018 & 0.041 & 0.041 & 0.046 & 0.046 \\\\\n",
            " % Residual Std. Error & 0.615 (df=3759) & 0.615 (df=3755) & 0.638 (df=2911) & 0.638 (df=2907) & 0.625 (df=6672) & 0.625 (df=6666) \\\\\n",
            " % F Statistic & 21.960$^{***}$ (df=32; 3759) & 21.858$^{***}$ (df=36; 3755) & 250.735$^{***}$ (df=32; 2911) & 65.033$^{***}$ (df=36; 2907) & 23.128$^{***}$ (df=63; 6672) & 21.830$^{***}$ (df=69; 6666) \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{6}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 37, but rank is 36\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 63, but rank is 62\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 71, but rank is 68\n",
            "  warnings.warn('covariance of constraints does not have full '\n"
          ]
        }
      ],
      "source": [
        "# creating latex table\n",
        "stargazer_tab4 = Stargazer(results_tab4)\n",
        "\n",
        "stargazer_tab4.custom_columns(['Y1', 'Y2','Pooled'], [2,2,2])\n",
        "stargazer_tab4.significant_digits(3)\n",
        "stargazer_tab4.rename_covariates({'treat12': 'Treat','treat13': 'Treat', 'treatMS1MS2': 'Treat', 'inter_R1': 'Treat - R1', 'inter_R2': 'Treat - R2', 'inter_R3': 'Treat - R3'})\n",
        "stargazer_tab4.covariate_order(['treat12', 'treat13', 'treatMS1MS2', 'inter_R1', 'inter_R2', 'inter_R3'])\n",
        "# adding p-values\n",
        "stargazer_tab4.add_line('P-Val Treat',p_values_df.loc['Treat'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat FWER',p_values_df_fwer.loc['Treat'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R1',p_values_df.loc['inter_R1'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R1 FWER',p_values_df_fwer.loc['inter_R1'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R2',p_values_df.loc['inter_R2'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R2 FWER',p_values_df_fwer.loc['inter_R2'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R3',p_values_df.loc['inter_R3'].tolist())\n",
        "stargazer_tab4.add_line('P-Val Treat - R3 FWER',p_values_df_fwer.loc['inter_R3'].tolist())\n",
        "\n",
        "\n",
        "latex_table4 = stargazer_tab4.render_latex()\n",
        "\n",
        "latex_table4 = latex_table4.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) \\n \\\\\\ & Overall & By rd & Overall & By rd & Overall & By rd \\\\\")\n",
        "latex_table4 = latex_table4.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table4 = latex_table4.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table4 = latex_table4.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table4 = latex_table4.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
        "latex_table4 = latex_table4.replace(\"nan\",\"\")\n",
        "latex_table4 = latex_table4.replace(\"logtotcons_trim\",\"Log Total Consumption\")\n",
        "latex_table4 = latex_table4.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
        "latex_table4 = latex_table4.replace(\"\\\\end{table}\", \"\")\n",
        "\n",
        "print(latex_table4)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFWCk0Ja1NUj"
      },
      "source": [
        "## Table 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBq_dQh4f4t"
      },
      "source": [
        "### Clean the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKjmh5Sc4Zuu"
      },
      "outputs": [],
      "source": [
        "ms1ms2_pooled_tab5 = ms1ms2_pooled.copy(deep=True)\n",
        "max_strata_group = ms1ms2_pooled_tab5['strata_group'].max()\n",
        "ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5['MS'] == 2, 'strata_group'] = ms1ms2_pooled_tab5['groupstrata'] + max_strata_group\n",
        "\n",
        "ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5['MS'] == 2, 'oafid'] = ms1ms2_pooled_tab5['fr_id']\n",
        "\n",
        "ms1ms2_pooled_tab5['purchasequant2'] = ms1ms2_pooled_tab5['purchasequant']\n",
        "ms1ms2_pooled_tab5.loc[(ms1ms2_pooled_tab5['purchaseval']==0)&(ms1ms2_pooled_tab5['purchasequant'].isna()),'purchasequant2'] = 0\n",
        "ms1ms2_pooled_tab5['netsales2'] = ms1ms2_pooled_tab5['salesquant'] - ms1ms2_pooled_tab5['purchasequant2']\n",
        "ms1ms2_pooled_tab5['netsales'] = ms1ms2_pooled_tab5['netsales2']\n",
        "\n",
        "ms1ms2_pooled_tab5.drop(columns=['netsales_trim','purchaseval_trim','salesval_trim'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CH5lm8Ib4bbO"
      },
      "outputs": [],
      "source": [
        "# trim outliers\n",
        "for x in ['purchaseval', 'salesval', 'purchasequant', 'salesquant']:\n",
        "    quantile = ms1ms2_pooled_tab5[x].quantile([0.99])\n",
        "    ms1ms2_pooled_tab5[f'{x}_trim'] = ms1ms2_pooled_tab5[x]\n",
        "    ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5[f'{x}_trim'] > quantile[0.99],f'{x}_trim'] = np.nan\n",
        "\n",
        "quantile = ms1ms2_pooled_tab5['netsales'].quantile([0.005, 0.995])\n",
        "ms1ms2_pooled_tab5['netsales_trim'] = ms1ms2_pooled_tab5['netsales']\n",
        "ms1ms2_pooled_tab5.loc[(ms1ms2_pooled_tab5['netsales_trim'] <= quantile[0.005]) | (ms1ms2_pooled_tab5['netsales_trim'] > quantile[0.995]) , 'netsales_trim'] = np.nan\n",
        "\n",
        "# create id\n",
        "ms1ms2_pooled_tab5['id'] = ms1ms2_pooled_tab5['oafid'].fillna(ms1ms2_pooled_tab5['fr_id'])\n",
        "\n",
        "# create effective prices\n",
        "trim_vars = ['salesquant_trim', 'purchasequant_trim', 'salesval_trim', 'purchaseval_trim']\n",
        "for var in trim_vars:\n",
        "    ms1ms2_pooled_tab5[f'tot_{var}'] = ms1ms2_pooled_tab5.groupby(['id', 'MS'])[var].transform('sum')\n",
        "\n",
        "for x in ['purchase', 'sales']:\n",
        "    ms1ms2_pooled_tab5[f'effective_{x}_price'] = ms1ms2_pooled_tab5[f'tot_{x}val_trim'] / ms1ms2_pooled_tab5[f'tot_{x}quant_trim']\n",
        "    ms1ms2_pooled_tab5.loc[ms1ms2_pooled_tab5[f'tot_{x}quant_trim']== 0,f'effective_{x}_price'] = np.nan"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NggiH6H4jkZ"
      },
      "source": [
        "### Net Sales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyNON6b14c0Q"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "\n",
        "# define variable\n",
        "dv = 'netsales_trim'\n",
        "independent_vars = ['z', 'treatMS1MS2_1 + treatMS1MS2_2 + treatMS1MS2_3']\n",
        "\n",
        "for i, var in enumerate(independent_vars):\n",
        "    df = ms1ms2_pooled_tab5.copy(deep=True)\n",
        "    df['z'] = df['treatMS1MS2']\n",
        "    if var == 'z':\n",
        "        df.dropna(subset=[dv,'z','interviewdate','Y1round2','Y1round3','Y2round1','Y2round2','Y2round3','strata_group','groupnum'], inplace=True)\n",
        "    else:\n",
        "        df.dropna(subset=[dv,'treatMS1MS2_1','treatMS1MS2_2','treatMS1MS2_3','interviewdate','Y1round2','Y1round3','Y2round1','Y2round2','Y2round3','strata_group','groupnum'], inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    formula = f'{dv} ~ {var} + interviewdate + Y1round2 + Y1round3 + Y2round1 + Y2round2 + Y2round3 + C(strata_group)'\n",
        "    model = smf.ols(formula, df).fit(cov_type='cluster', cov_kwds={'groups': df['groupnum']})\n",
        "    mean_dev = df.loc[df['treatMS1MS2'] == 0, dv].mean()\n",
        "    std_dev = df.loc[df['treatMS1MS2'] == 0, dv].std()\n",
        "    fwer_pvals = multipletests(model.pvalues, method='fdr_bh')[1]\n",
        "    results[f'netsales_{i}'] = {'model':model, 'mean_dev':mean_dev, 'std_dev':std_dev, 'fwer_pvals':fwer_pvals}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvFmwVf04lSt"
      },
      "source": [
        "### Effective Price"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhKee4CM4exT"
      },
      "outputs": [],
      "source": [
        "for dv in ['purchase', 'sales']:\n",
        "    for i, treat in enumerate(['treat12', 'treat13', 'treatMS1MS2']):\n",
        "        df = ms1ms2_pooled_tab5.copy(deep=True)\n",
        "        df['z'] = df[treat]\n",
        "        df = df.drop_duplicates(subset=['id', 'MS'], keep='first')\n",
        "        df.dropna(subset=[f'effective_{dv}_price','z','groupnum'], inplace=True)\n",
        "        if treat == 'treatMS1MS2':\n",
        "            formula = f'effective_{dv}_price ~ z + C(strata_group)'\n",
        "        else:\n",
        "            df = df[df['MS'] == i+1]\n",
        "            formula = f'effective_{dv}_price ~ z + C(strata_group)'\n",
        "        model = smf.ols(formula, data=df).fit(cov_type='cluster', cov_kwds={'groups': df['groupnum']})\n",
        "        mean_dev = df.loc[df['z'] == 0, f'effective_{dv}_price'].mean()\n",
        "        std_dev = df.loc[df['z'] == 0, f'effective_{dv}_price'].std()\n",
        "        fwer_pvals = multipletests(model.pvalues['z'], method='fdr_bh')[1]\n",
        "        results[f'{dv}_{treat}'] = {'model':model, 'mean_dev':mean_dev, 'std_dev':std_dev, 'fwer_pvals':fwer_pvals}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9CfKLPeP4nni"
      },
      "source": [
        "### LaTeX output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fnaff3b4uAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37f02317-8d92-4416-85f0-ca6087754ef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lcccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "\\\\[-1.8ex] & \\multicolumn{2}{c}{Net Sales} & \\multicolumn{2}{c}{Effective Price}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) \n",
            " \\\\ & Overall & By rd & Purchase & Sales \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Treat & 0.193$^{***}$ & & -57.449$^{**}$ & 145.509$^{***}$ \\\\\n",
            "& (0.064) & & (27.156) & (41.767) \\\\\n",
            " Treat - R1 & & -0.173$^{*}$ & & \\\\\n",
            "& & (0.095) & & \\\\\n",
            " Treat - R2 & & 0.376$^{***}$ & & \\\\\n",
            "& & (0.102) & & \\\\\n",
            " Treat - R3 & & 0.366$^{***}$ & & \\\\\n",
            "& & (0.091) & & \\\\\n",
            " P-value Treat &  &  & 0.034 & 0.0 \\\\\n",
            " P-value Treat FWER &  &  & 0.034 & 0.0 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 6736 & 6736 & 2014 & 1428 \\\\\n",
            " $R^2$ & 0.100 & 0.104 & 0.089 & 0.066 \\\\\n",
            " % Adjusted $R^2$ & 0.091 & 0.094 & 0.060 & 0.024 \\\\\n",
            " % Residual Std. Error & 1.998 & 1.994 & 639.432 & 789.099 \\\\\n",
            " % F Statistic & 21.288$^{***}$ & 19.708$^{***}$ & 34.249$^{***}$ & 13.002$^{***}$ \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{4}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 68, but rank is 66\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 70, but rank is 68\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 62, but rank is 61\n",
            "  warnings.warn('covariance of constraints does not have full '\n",
            "/usr/local/lib/python3.10/dist-packages/statsmodels/base/model.py:1894: ValueWarning: covariance of constraints does not have full rank. The number of constraints is 62, but rank is 61\n",
            "  warnings.warn('covariance of constraints does not have full '\n"
          ]
        }
      ],
      "source": [
        "models_list = ['netsales_0','netsales_1','purchase_treatMS1MS2','sales_treatMS1MS2']\n",
        "stargazer = Stargazer([results[model]['model'] for model in models_list])\n",
        "\n",
        "# get p-values\n",
        "pvals = np.round(pd.DataFrame({model:results[model]['model'].pvalues for model in ['purchase_treatMS1MS2','sales_treatMS1MS2']}),3)\n",
        "fwer_pvals = np.round(pd.DataFrame({model:results[model]['fwer_pvals'] for model in ['purchase_treatMS1MS2','sales_treatMS1MS2']}),3)\n",
        "# rename columns\n",
        "for i, df in enumerate([pvals, fwer_pvals]):\n",
        "    df.columns = ['Purchase', 'Sales']\n",
        "    df['Overall'] = \"\"\n",
        "    df['By rd'] = \"\"\n",
        "    # reorder columns\n",
        "    if i == 0:\n",
        "        pvals = df[['Overall', 'By rd', 'Purchase', 'Sales']]\n",
        "    else:\n",
        "        fwer_pvals = df[['Overall', 'By rd', 'Purchase', 'Sales']]\n",
        "\n",
        "# configure Stargazer object for output\n",
        "stargazer.custom_columns(['Net Sales', 'Effective Price'], [2, 2])\n",
        "stargazer.rename_covariates({'z': 'Treat','treatMS1MS2_1':'Treat - R1', 'treatMS1MS2_2':'Treat - R2', 'treatMS1MS2_3':'Treat - R3'})\n",
        "stargazer.show_degrees_of_freedom(False)\n",
        "stargazer.significant_digits(3)\n",
        "stargazer.covariate_order(['z', 'treatMS1MS2_1', 'treatMS1MS2_2', 'treatMS1MS2_3'])\n",
        "# add p-values as a rows\n",
        "stargazer.add_line('P-value Treat', pvals.loc['z'].values.tolist())\n",
        "stargazer.add_line('P-value Treat FWER', fwer_pvals.loc[0].values.tolist())\n",
        "\n",
        "latex_table5 = stargazer.render_latex()\n",
        "latex_table5 = latex_table5.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) \\n \\\\\\ & Overall & By rd & Purchase & Sales \\\\\")\n",
        "latex_table5 = latex_table5.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table5 = latex_table5.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table5 = latex_table5.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table5 = latex_table5.replace(\"\\\\textit{Note\",\"% \\\\textit{Note\")\n",
        "latex_table5 = latex_table5.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
        "latex_table5 = latex_table5.replace(\"\\\\end{table}\", \"\")\n",
        "\n",
        "print(latex_table5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA65rIF8J8Bc"
      },
      "source": [
        "## Table 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxbZzoqaJ373"
      },
      "outputs": [],
      "source": [
        "cleanpricedata_y1y2_tab6 = cleanpricedata_y1y2.copy(deep=True)\n",
        "cleanpricedata_y1y2_tab6 = cleanpricedata_y1y2_tab6[['salesPrice_trim','hi_1km_wt','hi_3km_wt','hi_5km_wt','monthnum','subloc_1km_wt_grp','subloc_3km_wt_grp','subloc_5km_wt_grp', 'in_sample','MS','lean']]\n",
        "cleanpricedata_y1y2_tab6['hi'] = pd.NA\n",
        "cleanpricedata_y1y2_tab6['interact'] = pd.NA\n",
        "cleanpricedata_y1y2_tab6['interact_lean'] = pd.NA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2T6HDAFA_Ji"
      },
      "outputs": [],
      "source": [
        "results = {}\n",
        "for dist in ['1km_wt', '3km_wt', '5km_wt']:\n",
        "    df = cleanpricedata_y1y2_tab6.copy(deep=True)\n",
        "    df.dropna(subset=[f'hi_{dist}','salesPrice_trim','monthnum'], inplace=True)\n",
        "    mean_price = df[(df['monthnum'] == 0) & (df[f'hi_{dist}'] == 0)]['salesPrice_trim'].mean()\n",
        "    norm = 100 / mean_price\n",
        "\n",
        "    # normalize price\n",
        "    df['salesPrice_trim_norm'] = df['salesPrice_trim'] * norm\n",
        "\n",
        "    # create hi variable\n",
        "    df['hi'] = df[f'hi_{dist}']\n",
        "    df['interact'] = df['monthnum'] * df['hi']\n",
        "\n",
        "    # regression\n",
        "    formula = 'salesPrice_trim_norm ~ hi + monthnum + interact'\n",
        "\n",
        "    for ms in [1,2,3]: # 3 is pooled\n",
        "        if ms == 3:\n",
        "            df_filt = df[(df['in_sample'] == 1)]\n",
        "        else:\n",
        "            df_filt = df[(df['MS'] == ms) & (df['in_sample'] == 1)]\n",
        "        model = smf.ols(formula=formula, data=df_filt).fit(cov_type='cluster', cov_kwds={'groups': df_filt[f'subloc_{dist}_grp']})\n",
        "        results[(dist, ms)] = model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSZoETpyBBMy"
      },
      "outputs": [],
      "source": [
        "pvals = pd.DataFrame()\n",
        "# storeing pval in a df\n",
        "for dv in ['hi', 'monthnum', 'interact']:\n",
        "    val = {(k[0], k[1]): np.round(v.pvalues[dv],3) for k, v in results.items()}\n",
        "    pvals[dv] = pd.Series(val)\n",
        "\n",
        "# keep only columns 3km_wt and 3rd column in 1km_wt and 5km_wt\n",
        "pvals = pvals.T\n",
        "pvals = pvals[[('3km_wt', 1), ('3km_wt', 2), ('3km_wt', 3), ('1km_wt', 3), ('5km_wt', 3)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5ZMLa8PBDSp"
      },
      "outputs": [],
      "source": [
        "def wild_bootstrap(data, model, n_bootstraps, dv, clust_var):\n",
        "    \"\"\"\n",
        "    Wild Cluster Bootstrap-t with random signs within clusters\n",
        "    \"\"\"\n",
        "    cluster_var = data[clust_var]\n",
        "    unique_clusters = cluster_var.unique()\n",
        "    boot_results = []\n",
        "\n",
        "    for _ in range(n_bootstraps):\n",
        "        boot_data = data.copy()\n",
        "        # resample residuals within each cluster\n",
        "        for cluster in unique_clusters:\n",
        "            cluster_indices = data[cluster_var == cluster].index\n",
        "\n",
        "            # multiply residuals by random signs, either -1 or 1, within each cluster\n",
        "            signs = np.random.choice([-1, 1], size=len(cluster_indices))\n",
        "            boot_data.loc[cluster_indices, dv] = model.predict(data.loc[cluster_indices]) + signs * model.resid.loc[cluster_indices]\n",
        "\n",
        "        # Refit model on bootstrapped data\n",
        "        boot_model = smf.ols(model.model.formula, data=boot_data).fit()\n",
        "        boot_results.append(boot_model.params)\n",
        "\n",
        "    return np.array(boot_results)[:,1:4]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQQIwPu2BFpk"
      },
      "outputs": [],
      "source": [
        "bootstrap_res = {}\n",
        "bootstrap_pvals = pd.DataFrame(index=pd.MultiIndex.from_product([['1km_wt', '3km_wt', '5km_wt'], [1, 2, 3]], names=['dist', 'ms']), columns=['hi', 'monthnum', 'interact'])\n",
        "n_bootstraps = 999\n",
        "\n",
        "for dist  in ['1km_wt', '3km_wt', '5km_wt']:\n",
        "    df = cleanpricedata_y1y2_tab6.copy(deep=True)\n",
        "    df.dropna(subset=[f'hi_{dist}','salesPrice_trim','monthnum'], inplace=True)\n",
        "    mean_price = df[(df['monthnum'] == 0) & (df[f'hi_{dist}'] == 0)]['salesPrice_trim'].mean()\n",
        "    norm = 100 / mean_price\n",
        "\n",
        "    # normalize price\n",
        "    df['salesPrice_trim_norm'] = df['salesPrice_trim'] * norm\n",
        "    df['salesPrice_trim_norm'] = df['salesPrice_trim_norm'].astype(float)\n",
        "\n",
        "    # create hi variable\n",
        "    df['hi'] = df[f'hi_{dist}']\n",
        "    df['interact'] = df['monthnum'] * df['hi']\n",
        "\n",
        "    # regression\n",
        "    formula = 'salesPrice_trim_norm ~ hi + monthnum + interact'\n",
        "\n",
        "\n",
        "    for ms in [1,2,3]: # 3 is pooled\n",
        "        if ms == 3:\n",
        "            df_filt = df[(df['in_sample'] == 1)]\n",
        "        else:\n",
        "            df_filt = df[(df['MS'] == ms) & (df['in_sample'] == 1)]\n",
        "        res = wild_bootstrap(df_filt, results[(dist, ms)], n_bootstraps, 'salesPrice_trim_norm', f'subloc_{dist}_grp')\n",
        "        bootstrap_res[(dist,ms)] = res\n",
        "\n",
        "        model = results[(dist, ms)]\n",
        "\n",
        "        for i, var in enumerate(['hi', 'monthnum', 'interact']):\n",
        "            observed_coef = model.params[var]\n",
        "            # calculating p-values as proportion of bootstrap coefs where abs(boot_coef) >= abs(obs_coef)\n",
        "            p_value = np.round(np.mean(np.abs(bootstrap_res[(dist,ms)][:,i]) >= np.abs(observed_coef)),3)\n",
        "\n",
        "            # store p-value in df\n",
        "            bootstrap_pvals.loc[(dist,ms),var] = p_value\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUQzZYEiBI4h"
      },
      "outputs": [],
      "source": [
        "bootstrap_pvals = bootstrap_pvals.T\n",
        "bootstrap_pvals = bootstrap_pvals[[('3km_wt', 1), ('3km_wt', 2), ('3km_wt', 3), ('1km_wt', 3), ('5km_wt', 3)]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fv9ZnKRtBNWU",
        "outputId": "e3954305-1a3c-420f-cc26-66a8587fc032"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lccccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "& \\multicolumn{5}{c}{% \\textit{Dependent variable: salesPrice_trim_norm}} \\\n",
            "\\cr \\cline{2-6}\n",
            "\\\\[-1.8ex] & \\multicolumn{3}{c}{Main Specification (3km)} & \\multicolumn{2}{c}{Robustness (Pooled)}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \n",
            " \\\\ & Y1 & Y2 & Pooled & 1km & 5km \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " High & 4.410$^{**}$ & 2.855$^{}$ & 3.970$^{**}$ & 2.787$^{}$ & 3.766$^{**}$ \\\\\n",
            "& (2.091) & (1.992) & (1.817) & (1.719) & (1.822) \\\\\n",
            " Month & 1.189$^{***}$ & 1.224$^{***}$ & 1.364$^{***}$ & 1.327$^{***}$ & 1.537$^{***}$ \\\\\n",
            "& (0.363) & (0.377) & (0.350) & (0.339) & (0.291) \\\\\n",
            " High x Month & -0.574$^{}$ & -0.476$^{}$ & -0.573$^{}$ & -0.520$^{}$ & -0.835$^{**}$ \\\\\n",
            "& (0.422) & (0.459) & (0.386) & (0.390) & (0.366) \\\\\n",
            " P-value High & 0.035 & 0.152 & 0.029 & 0.105 & 0.039 \\\\\n",
            " P-value Treat Bootstrap & 0.474 & 0.528 & 0.49 & 0.474 & 0.505 \\\\\n",
            " P-value Month & 0.001 & 0.001 & 0.0 & 0.0 & 0.0 \\\\\n",
            " P-value High Bootstrap & 0.484 & 0.514 & 0.51 & 0.483 & 0.517 \\\\\n",
            " P-value High x Month & 0.173 & 0.3 & 0.138 & 0.182 & 0.023 \\\\\n",
            " P-value Treat x High Bootstrap & 0.483 & 0.622 & 0.498 & 0.489 & 0.505 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 491 & 381 & 872 & 872 & 872 \\\\\n",
            " $R^2$ & 0.077 & 0.031 & 0.058 & 0.055 & 0.060 \\\\\n",
            " % Adjusted $R^2$ & 0.071 & 0.023 & 0.055 & 0.052 & 0.056 \\\\\n",
            " % Residual Std. Error & 10.071 & 14.651 & 12.700 & 12.726 & 12.685 \\\\\n",
            " % F Statistic & 6.401$^{***}$ & 7.496$^{***}$ & 13.411$^{***}$ & 10.971$^{***}$ & 16.730$^{***}$ \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# use stargazer to create a table\n",
        "result_list = [results[('3km_wt', 1)], results[('3km_wt', 2)], results[('3km_wt', 3)], results[('1km_wt', 3)], results[('5km_wt', 3)]]\n",
        "stargazer = Stargazer(result_list)\n",
        "\n",
        "# configure Stargazer object for output\n",
        "stargazer.custom_columns(['Main Specification (3km)', 'Robustness (Pooled)'], [3, 2])\n",
        "stargazer.rename_covariates({'hi': 'High', 'monthnum': 'Month', 'interact': 'High x Month'})\n",
        "stargazer.show_degrees_of_freedom(False)\n",
        "stargazer.significant_digits(3)\n",
        "stargazer.covariate_order(['hi', 'monthnum', 'interact'])\n",
        "# add p-values as a rows\n",
        "stargazer.add_line('P-value High', pvals.loc['hi'].values.tolist())\n",
        "stargazer.add_line('P-value Treat Bootstrap', bootstrap_pvals.loc['hi'].values.tolist())\n",
        "stargazer.add_line('P-value Month', pvals.loc['monthnum'].values.tolist())\n",
        "stargazer.add_line('P-value High Bootstrap', bootstrap_pvals.loc['monthnum'].values.tolist())\n",
        "stargazer.add_line('P-value High x Month', pvals.loc['interact'].values.tolist())\n",
        "stargazer.add_line('P-value Treat x High Bootstrap', bootstrap_pvals.loc['interact'].values.tolist())\n",
        "\n",
        "\n",
        "latex_table6 = stargazer.render_latex()\n",
        "\n",
        "# edit the latex table to add row for telling if Y1 Y2 or Pooled after \\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\n",
        "latex_table6 = latex_table6.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) \\n \\\\\\ & Y1 & Y2 & Pooled & 1km & 5km \\\\\")\n",
        "latex_table6 = latex_table6.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table6 = latex_table6.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table6 = latex_table6.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table6 = latex_table6.replace(\"\\\\textit{\",\"% \\\\textit{\")\n",
        "latex_table6 = latex_table6.replace(\"\\\\begin{table}[!htbp] \\\\centering\", \"\")\n",
        "latex_table6 = latex_table6.replace(\"\\\\end{table}\", \"\")\n",
        "\n",
        "print(latex_table6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXsVtSIyyW_w"
      },
      "source": [
        "## Table 7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRr_Mp-DyYTi"
      },
      "outputs": [],
      "source": [
        "# copy the raw data and create columns for treatment and interaction variable\n",
        "ms1ms2_pooled_tab7 = ms1ms2_pooled.copy(deep=True)\n",
        "# filter relevant columns\n",
        "ms1ms2_pooled_tab7 = ms1ms2_pooled_tab7[['oafid', # id\n",
        "                                         'treat12', 'treat13', 'treatMS1MS2', # treatment variables\n",
        "                                         'inventory_trim', 'netrevenue_trim', 'logtotcons_trim', # outcome variables\n",
        "                                         'Y1round2', 'Y1round3', 'Y2round1', 'Y2round2', 'Y2round3','hi','subloc','interviewdate']] # independent variables\n",
        "\n",
        "ms1ms2_pooled_tab7.sort_index(inplace=True)\n",
        "ms1ms2_pooled_tab7['z'] = pd.NA\n",
        "ms1ms2_pooled_tab7['z_hi'] = pd.NA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EjJ8Mj1g3ND"
      },
      "source": [
        "### Running the first set of regressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tb7xZ1W4yiA9"
      },
      "outputs": [],
      "source": [
        "# list of treaments\n",
        "treatments = ['treat12', 'treat13', 'treatMS1MS2']\n",
        "\n",
        "# list of dependent variables\n",
        "dependent_vars = ['inventory_trim', 'netrevenue_trim', 'logtotcons_trim']\n",
        "\n",
        "# list of changeing independent variables depending on the treatment\n",
        "independent_vars = {\n",
        "    'treat12': 'Y1round2 + Y1round3',\n",
        "    'treat13': 'Y2round2 + Y2round3',\n",
        "    'treatMS1MS2': 'Y1round2 + Y1round3 + Y2round1 + Y2round2 + Y2round3'\n",
        "    }\n",
        "\n",
        "# empty dictionary to store results\n",
        "results = {}\n",
        "pvals = {var: [] for var in ['z', 'hi', 'z_hi','z+z_hi']}\n",
        "\n",
        "# Simulating the loop to replace variables and run regressions\n",
        "for dv in dependent_vars:\n",
        "    for treat in treatments:\n",
        "        # Stata automatically omits the missing values in the regression – here we have to do it manually so we copy the data and drop variables\n",
        "        df = ms1ms2_pooled_tab7.copy(deep=True)\n",
        "        df = df.dropna(subset=[dv, treat, 'hi', 'subloc','interviewdate'])\n",
        "        # setting treament variable\n",
        "        df['z'] = df[treat] # setting z to the treatment variable\n",
        "\n",
        "        # setting interaction variable\n",
        "        df['z_hi'] = df[treat]*df['hi'] # setting z_hi to the interaction of the treatment hi saturation\n",
        "\n",
        "        # setting the formula to run the regression\n",
        "        formula = f'{dv} ~ z + hi + z_hi + interviewdate + {independent_vars[treat]}'\n",
        "\n",
        "        # Run the regression\n",
        "        model_key = f'model_{dependent_vars.index(dv)*len(treatments) + treatments.index(treat)}'\n",
        "        results[model_key] = smf.ols(formula, data=df).fit(cov_type='cluster', cov_kwds={'groups': df['subloc']})\n",
        "        # print(results[f'model_{i}'].summary())\n",
        "\n",
        "        # test the hypothesis that z + z_hi = 0\n",
        "        hypothesis = 'z + z_hi = 0'\n",
        "        t_test = results[model_key].t_test(hypothesis)\n",
        "\n",
        "        # store p-value round to 3 decimals\n",
        "        pvals['z+z_hi'].append(np.round(t_test.pvalue,3))\n",
        "        pvals['z'].append(np.round(results[model_key].pvalues['z'],3))\n",
        "        pvals['hi'].append(np.round(results[model_key].pvalues['hi'],3))\n",
        "        pvals['z_hi'].append(np.round(results[model_key].pvalues['z_hi'],3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOjQSC-NhbD7"
      },
      "source": [
        "### Running boostrap regressions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtNR18qGhgM5"
      },
      "outputs": [],
      "source": [
        "n_bootstraps = 999  # reported data is based on 999 iterations\n",
        "bootstrap_res = {}\n",
        "bootstrap_pvals = {var: [] for var in ['z', 'hi', 'z_hi']}\n",
        "\n",
        "for dv in dependent_vars:\n",
        "    for treat in treatments:\n",
        "        df = ms1ms2_pooled_tab7.copy(deep=True)\n",
        "        df = df.dropna(subset=[dv, treat, 'hi', 'subloc', 'interviewdate'])\n",
        "        df['z'] = df[treat]\n",
        "        df['z_hi'] = df[treat] * df['hi']\n",
        "        df[dv] = df[dv].astype(float)\n",
        "\n",
        "        formula = f'{dv} ~ z + hi + z_hi + interviewdate + {independent_vars[treat]}'\n",
        "        model_key = f'model_{dependent_vars.index(dv)*len(treatments) + treatments.index(treat)}'\n",
        "        model = results[model_key]\n",
        "\n",
        "        # Wild bootstrap\n",
        "        res = wild_bootstrap(df, model, n_bootstraps, dv,'subloc')\n",
        "        bootstrap_res[model_key] = res\n",
        "\n",
        "        for i, var in enumerate(['z', 'hi', 'z_hi']):\n",
        "            observed_coef = model.params[var]\n",
        "            # calculating p-values as proportion of bootstrap coefs where abs(boot_coef) >= abs(obs_coef)\n",
        "            p_value = np.round(np.mean(np.abs(bootstrap_res[model_key][:,i]) >= np.abs(observed_coef)),3)\n",
        "            bootstrap_pvals[var].append(p_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UdXeJZAhhEo"
      },
      "source": [
        "### Output to LaTeX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOQVwnBvwMBm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fce43d81-5bfc-4e08-a04a-b2d2d246374a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{table}[!htbp] \\centering\n",
            "\\begin{tabular}{@{\\extracolsep{5pt}}lccccccccc}\n",
            "\\\\[-1.8ex]\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "\\\\[-1.8ex] & \\multicolumn{3}{c}{Inventory} & \\multicolumn{3}{c}{Net Revenues} & \\multicolumn{3}{c}{Consumption}  \\\\\n",
            "\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \n",
            " \\\\ & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Treat & 0.759$^{***}$ & 0.546$^{***}$ & 0.740$^{***}$ & 1059.602$^{**}$ & 1193.768$^{*}$ & 1101.389$^{**}$ & 0.012$^{}$ & -0.051$^{}$ & -0.011$^{}$ \\\\\n",
            "& (0.189) & (0.185) & (0.155) & (437.732) & (685.048) & (430.091) & (0.040) & (0.040) & (0.023) \\\\\n",
            " High & 0.124$^{}$ & -0.028$^{}$ & 0.017$^{}$ & 533.903$^{}$ & -152.603$^{}$ & 164.936$^{}$ & -0.003$^{}$ & -0.084$^{}$ & -0.047$^{}$ \\\\\n",
            "& (0.355) & (0.219) & (0.241) & (551.179) & (558.948) & (479.685) & (0.051) & (0.053) & (0.043) \\\\\n",
            " Treat*High & -0.333$^{}$ & -0.065$^{}$ & -0.291$^{}$ & -1114.628$^{**}$ & -555.215$^{}$ & -816.770$^{}$ & -0.013$^{}$ & 0.174$^{***}$ & 0.067$^{*}$ \\\\\n",
            "& (0.229) & (0.255) & (0.192) & (535.594) & (804.864) & (520.036) & (0.052) & (0.055) & (0.037) \\\\\n",
            " P-value T + TH = 0 & 0.002 & 0.006 & 0.002 & 0.861 & 0.125 & 0.396 & 0.969 & 0.001 & 0.063 \\\\\n",
            " P-value Treat & 0.0 & 0.003 & 0.0 & 0.015 & 0.081 & 0.01 & 0.764 & 0.209 & 0.621 \\\\\n",
            " P-value Treat Bootstrap & 0.5185185185185185 & 0.4904904904904905 & 0.4804804804804805 & 0.5005005005005005 & 0.5245245245245245 & 0.5115115115115115 & 0.7757757757757757 & 0.5365365365365365 & 0.7387387387387387 \\\\\n",
            " P-value High & 0.726 & 0.899 & 0.944 & 0.333 & 0.785 & 0.731 & 0.961 & 0.115 & 0.279 \\\\\n",
            " P-value High Bootstrap & 0.6106106106106106 & 0.8678678678678678 & 0.8918918918918919 & 0.4804804804804805 & 0.7197197197197197 & 0.6106106106106106 & 0.9319319319319319 & 0.4854854854854855 & 0.5045045045045045 \\\\\n",
            " P-value Treat*High & 0.146 & 0.799 & 0.13 & 0.037 & 0.49 & 0.116 & 0.798 & 0.002 & 0.072 \\\\\n",
            " P-value Treat*High Bootstrap & 0.5145145145145145 & 0.7617617617617618 & 0.48348348348348347 & 0.4964964964964965 & 0.5135135135135135 & 0.4984984984984985 & 0.7927927927927928 & 0.5205205205205206 & 0.5175175175175175 \\\\\n",
            "\\hline \\\\[-1.8ex]\n",
            " Observations & 3836 & 2944 & 6780 & 3795 & 2935 & 6730 & 3792 & 2944 & 6736 \\\\\n",
            " $R^2$ & 0.346 & 0.184 & 0.293 & 0.009 & 0.043 & 0.091 & 0.002 & 0.017 & 0.025 \\\\\n",
            " % Adjusted $R^2$ & 0.345 & 0.182 & 0.292 & 0.008 & 0.041 & 0.090 & 0.000 & 0.015 & 0.024 \\\\\n",
            " % Residual Std. Error & 3.015 & 2.793 & 2.947 & 6188.647 & 6410.741 & 6286.767 & 0.621 & 0.647 & 0.633 \\\\\n",
            " % F Statistic & 369.556$^{***}$ & 93.029$^{***}$ & 364.779$^{***}$ & 2.004$^{*}$ & 19.627$^{***}$ & 119.335$^{***}$ & 0.616$^{}$ & 4.496$^{***}$ & 16.477$^{***}$ \\\\\n",
            "\\hline\n",
            "\\hline \\\\[-1.8ex]\n",
            "% \\textit{Note:} & \\multicolumn{9}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\\\\n",
            "\\end{tabular}\n",
            "\\end{table}\n"
          ]
        }
      ],
      "source": [
        "# use stargazer to create a table\n",
        "result_list = list(results.values())\n",
        "stargazer = Stargazer(result_list)\n",
        "\n",
        "# configure Stargazer object for output\n",
        "stargazer.custom_columns(['Inventory', 'Net Revenues', 'Consumption'], [3, 3, 3])\n",
        "stargazer.rename_covariates({'z': 'Treat', 'hi': 'High', 'z_hi': 'Treat*High'})\n",
        "stargazer.show_degrees_of_freedom(False)\n",
        "stargazer.significant_digits(3)\n",
        "stargazer.covariate_order(['z', 'hi', 'z_hi'])\n",
        "# add p-values as a rows\n",
        "stargazer.add_line('P-value T + TH = 0', pvals['z+z_hi'])\n",
        "stargazer.add_line('P-value Treat', pvals['z'])\n",
        "stargazer.add_line('P-value Treat Bootstrap', bootstrap_pvals['z'])\n",
        "stargazer.add_line('P-value High', pvals['hi'])\n",
        "stargazer.add_line('P-value High Bootstrap', bootstrap_pvals['hi'])\n",
        "stargazer.add_line('P-value Treat*High', pvals['z_hi'])\n",
        "stargazer.add_line('P-value Treat*High Bootstrap', bootstrap_pvals['z_hi'])\n",
        "\n",
        "\n",
        "latex_table7 = stargazer.render_latex()\n",
        "\n",
        "# edit the latex table to add row for telling if Y1 Y2 or Pooled after \\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\n",
        "latex_table7 = latex_table7.replace(\"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\\\\",\n",
        "                                \"\\\\[-1.8ex] & (1) & (2) & (3) & (4) & (5) & (6) & (7) & (8) & (9) \\n \\\\\\ & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled & Y1 & Y2 & Pooled \\\\\")\n",
        "latex_table7 = latex_table7.replace(\"Adjusted $R^2$\", \"% Adjusted $R^2$\")\n",
        "latex_table7 = latex_table7.replace(\"Residual Std. Error\", \"% Residual Std. Error\")\n",
        "latex_table7 = latex_table7.replace(\"F Statistic\", \"% F Statistic\")\n",
        "latex_table7 = latex_table7.replace(\"\\\\textit{\",\"% \\\\textit{\")\n",
        "\n",
        "print(latex_table7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F11Z5SCUQyO_"
      },
      "source": [
        "## Table 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE0tG6KRQ0xg"
      },
      "outputs": [],
      "source": [
        "tab8_dt = ms1ms2_pooled.loc[:, ['treatMS1MS2', 'hi', 'treatMS1MS2hi', 'interviewdate', 'netrevenue_trim', 'strata_group', 'groupnum', 'subloc', 'Y1round1', 'Y1round2', 'Y1round3', 'Y2round1', 'Y2round2', 'Y2round3']].dropna()\n",
        "tab8_dt['net_revenue_3'] = tab8_dt['netrevenue_trim'] * 3\n",
        "model = smf.ols('net_revenue_3 ~ treatMS1MS2 + hi + treatMS1MS2hi + interviewdate + Y1round1 + Y1round2 + Y1round3 + Y2round1 + Y2round2 + Y2round3', data=tab8_dt)\n",
        "results_t8 = model.fit(cov_type='cluster', cov_kwds={'groups': tab8_dt['subloc']})\n",
        "model_params_t8 = results_t8.params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocrbGi-hmfA",
        "outputId": "7419fd56-d92e-48a2-8af6-51fc5ebb7621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\\begin{tabular}{lrr}\n",
            "\\toprule\n",
            " & Low Saturation & High Saturation \\\\\n",
            "\\midrule\n",
            "1. Direct gains/HH (Ksh) & 3304.166 & 853.856 \\\\\n",
            "2. Indirect gains/HH (Ksh) & 0.000 & 494.807 \\\\\n",
            "3. Ratio of indirect to direct gains & 0.000 & 0.579 \\\\\n",
            "4. Direct beneficiary population (HH) & 247.000 & 495.000 \\\\\n",
            "5. Total local population (HH) & 3553.000 & 3553.000 \\\\\n",
            "6. Total direct gains (Ksh) & 816128.985 & 422658.745 \\\\\n",
            "7. Total indirect gains (Ksh) & 0.000 & 1758050.851 \\\\\n",
            "8. Total gains (direct + indirect; Ksh) & 816128.985 & 2180709.596 \\\\\n",
            "9. Fraction of gains direct & 1.000 & 0.194 \\\\\n",
            "10. Fraction of gains indirect & 0.000 & 0.806 \\\\\n",
            "\\bottomrule\n",
            "\\end{tabular}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Annualized coefficients\n",
        "treat_coef = model_params_t8['treatMS1MS2']\n",
        "treat_hi_coef = (model_params_t8['treatMS1MS2'] + model_params_t8['treatMS1MS2hi'])\n",
        "hi_coef = model_params_t8['hi']\n",
        "\n",
        "# Direct beneficiary population\n",
        "direct_beneficiary_pop_low = 247.0\n",
        "direct_beneficiary_pop_high = 495.0\n",
        "\n",
        "# Total direct gains\n",
        "total_direct_gains_low = treat_coef * direct_beneficiary_pop_low\n",
        "total_direct_gains_high = treat_hi_coef * direct_beneficiary_pop_high\n",
        "\n",
        "# Total indirect gains (only applicable to high saturation areas)\n",
        "total_indirect_gains_high = hi_coef * 3553.0\n",
        "\n",
        "# Total gains\n",
        "total_gains_low = total_direct_gains_low\n",
        "total_gains_high = total_direct_gains_high + total_indirect_gains_high\n",
        "\n",
        "# Fraction of gains direct\n",
        "fraction_gains_direct_low = 1  # All gains are direct in low saturation\n",
        "fraction_gains_direct_high = total_direct_gains_high / total_gains_high\n",
        "\n",
        "# Fraction of gains indirect (only applicable to high saturation areas)\n",
        "fraction_gains_indirect_high = total_indirect_gains_high / total_gains_high\n",
        "\n",
        "table_8 = {\n",
        "    \"1. Direct gains/HH (Ksh)\": [treat_coef, treat_hi_coef],\n",
        "    \"2. Indirect gains/HH (Ksh)\": [0, hi_coef],\n",
        "    \"3. Ratio of indirect to direct gains\": [0, hi_coef / treat_hi_coef],\n",
        "    \"4. Direct beneficiary population (HH)\": [direct_beneficiary_pop_low, direct_beneficiary_pop_high],\n",
        "    \"5. Total local population (HH)\": [3553.0, 3553.0],\n",
        "    \"6. Total direct gains (Ksh)\": [total_direct_gains_low, total_direct_gains_high],\n",
        "    \"7. Total indirect gains (Ksh)\": [0, total_indirect_gains_high],\n",
        "    \"8. Total gains (direct + indirect; Ksh)\": [total_gains_low, total_gains_high],\n",
        "    \"9. Fraction of gains direct\": [fraction_gains_direct_low, fraction_gains_direct_high],\n",
        "    \"10. Fraction of gains indirect\": [0, fraction_gains_indirect_high],\n",
        "}\n",
        "\n",
        "# Convert the calculations to DataFrame and transpose it\n",
        "table_8_df = pd.DataFrame(table_8, index=[\"Low Saturation\", \"High Saturation\"]).T\n",
        "\n",
        "# Now you can print table_8_df to see the recreated table\n",
        "table_8_df\n",
        "\n",
        "latex_table8 = table_8_df.to_latex(index=True, float_format=\"%.3f\")\n",
        "print(latex_table8)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "_0oyPnEQIxpq",
        "pgzJ03RhI4Go",
        "NLR-EsZmPhnZ",
        "J2Kt7J_ve0Jr",
        "MapKqTx56PLs",
        "EFcatyv4Dk08",
        "iFWCk0Ja1NUj",
        "BA65rIF8J8Bc",
        "eXsVtSIyyW_w",
        "F11Z5SCUQyO_"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}